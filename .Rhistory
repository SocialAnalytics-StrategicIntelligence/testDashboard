contrib_zipMap=rbind(contrib_zipMap2012,contrib_zipMap2016)
# this will give us trouble!
base + geom_sf(data=contrib_zipMap,
aes(fill=amount_perCap_DEMOCRAT_cat)) +
scale_fill_brewer(palette = "YlOrRd") +
facet_grid(~election_year,space = )
map2012=base + geom_sf(data=contrib_zipMap[contrib_zipMap$election_year==2012,],
aes(fill=amount_perCap_DEMOCRAT_cat)) +
scale_fill_brewer(palette = "YlOrRd")
map2016=base + geom_sf(data=contrib_zipMap[contrib_zipMap$election_year==2016,],
aes(fill=amount_perCap_DEMOCRAT_cat)) +
scale_fill_brewer(palette = "YlOrRd")
gridExtra::grid.arrange(map2012,map2016,ncol=1)
DEMmap= base + geom_sf(data=contrib_zipMap,
aes(fill=amount_perCap_DEMOCRAT_cat_labels)) +
scale_fill_brewer(palette = "PuBuGn") +
labs(fill="DEM contributions perCap") +
facet_wrap(~election_year)
DEMmap
library(raster)
mapCRS=crs(zipMap) # projection of our map
contriWA_1216_RD_mapPoints=st_as_sf(contriWA_1216_RD,# data frame
coords = c("Lon","Lat"), # form the data frame
remove = TRUE, # remove lon/lat
crs=mapCRS) # projection for spatial object
names(contriWA_1216_RD_mapPoints)
class(contriWA_1216_RD_mapPoints)
library(tmaptools)
get_proj4(crs(contriWA_1216_RD_mapPoints))
# clean memory
rm(list = ls())
link1="https://github.com/EvansDataScience/VAforPM_Text/"
link2="raw/main/trumps.csv"
trumpLink=paste0(link1,link2)
allTweets=read.csv(trumpLink ,stringsAsFactors = F)
names(allTweets)
head(allTweets$text,2)
DTtweets=allTweets[allTweets$is_retweet==FALSE ,]
DTtweets=DTtweets[,c(1,2)]
#some
head(DTtweets,3)
library(tidytext)
library(magrittr)
DTtweets_Words = DTtweets %>%
unnest_tokens(output=EachWord, # column created
input=text,# input column from DTtweets
token="words") # level of unnesting
head(DTtweets_Words,10) # notice 'EachWord'
nrow(DTtweets_Words) # count of words
# calling the file
data(stop_words)
# seeing some 'STOP WORDS'
head(stop_words)
library(dplyr)
# The column 'word' from 'stop_words' will be compared # to the column 'EachWord' in 'DTtweets_Words'
DTtweets_Words = DTtweets_Words %>%anti_join(stop_words,
by = c("EachWord" = "word"))
# You have these many rows now:
nrow(DTtweets_Words) # count of words
forCloud=as.data.frame(table(DTtweets_Words$EachWord))
names(forCloud)=c('EachWord','Counts')
#sorting by count:
forCloud_ascending=forCloud[order(forCloud$Counts),]
head(forCloud_ascending,10)
tail(forCloud_ascending,10)
# dropping by text:
badWords=c('https','t.co')
forCloud_ascending=forCloud_ascending[!forCloud_ascending$EachWord%in%badWords,]
# dropping by count:
forCloud_ascending=forCloud_ascending[forCloud_ascending$Counts>4,]
library(ggwordcloud)
ggplot(forCloud_ascending,
aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area() +
theme_minimal() +
scale_color_gradient(low = "red", high = "darkred")
ggplot(forCloud_ascending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.65) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.65) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
otherText <- read.delim("sometext.txt",header = F)
otherText_words=otherText %>%
unnest_tokens(output=EachWord,
input=V1,
token="words")%>%
anti_join(stop_words,
by = c("EachWord" = "word"))
# you get
head(otherText_words,20)
txtAsc_descending=dplyr::count(otherText_words,EachWord,
name='Counts',
sort = TRUE)
# result
head(txtAsc_descending,10)
# let's subset:
txtAsc_descending=txtAsc_descending[txtAsc_descending$Counts>4,]
ggplot(txtAsc_descending,
aes(label = EachWord,
size = Counts,color = Counts)) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal() +
scale_color_gradient(low = "red", high = "darkred")
txtAsc_descending
ggplot(txtAsc_descending,
aes(label = EachWord)  + geom_bar()
txtAsc_descending
ggplot(txtAsc_descending)  + geom_bar(aes(x=EachWord,y=Counts))
ggplot(txtAsc_descending)  + geom_bar(aes(x=EachWord,y=Counts),stat = 'identity')
ggplot(txtAsc_descending)  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity')
ggplot(txtAsc_descending)  +
geom_bar(aes(x=reorder(EachWord,-Counts),y=Counts),stat = 'identity')
ggplot(txtAsc_descending)  +
geom_bar(aes(x=reorder(EachWord,-Counts),y=Counts),stat = 'identity') + coord_flip()
ggplot(txtAsc_descending)  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip()
ggplot(head(txtAsc_descending,20))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip()
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip()
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="")
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(Counts))
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=Counts))
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=Counts, x=EachWord,y=Counts))
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=Counts, x=EachWord,y=Counts),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=Counts, x=EachWord,y=Counts/sum(Counts)),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=Counts/sum(Counts), x=EachWord,y=Counts),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=round(Counts/sum(Counts),1), x=EachWord,y=Counts),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=round(Counts/sum(Counts),2), x=EachWord,y=Counts),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=100*round(Counts/sum(Counts),2), x=EachWord,y=Counts),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=100*round(Counts/sum(Counts),3), x=EachWord,y=Counts),nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=paste0(100*round(Counts/sum(Counts),3),'%'),
x=EachWord,
y=Counts),
nudge_y = 1)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=paste0(100*round(Counts/sum(Counts),3),'%'),
x=EachWord,
y=Counts),
nudge_y = 1.5)
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=paste0(100*round(Counts/sum(Counts),3),'%'),
x=EachWord,
y=Counts),
nudge_y = 2)
otherText <- read.delim("https://github.com/DACSS-Visual/basicText/raw/main/sometext.txt",header = F)
otherText
otherText_words=otherText %>%
unnest_tokens(output=EachWord,
input=V1,
token="words")%>%
anti_join(stop_words,
by = c("EachWord" = "word"))
# you get
head(otherText_words,20)
txtAsc_descending=dplyr::count(otherText_words,EachWord,
name='Counts',
sort = TRUE)
# result
head(txtAsc_descending,10)
# let's subset:
txtAsc_descending=txtAsc_descending[txtAsc_descending$Counts>4,]
ggplot(txtAsc_descending,
aes(label = EachWord,
size = Counts,color = Counts)) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal() +
scale_color_gradient(low = "red", high = "darkred")
ggplot(txtAsc_descending)  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip()
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=paste0(100*round(Counts/sum(Counts),3),'%'),
x=EachWord,
y=Counts),
nudge_y = 2)
# clean memory
rm(list = ls())
link1="https://github.com/DACSS-Visual/basicText/"
link2="raw/main/trumps.csv"
trumpLink=paste0(link1,link2)
allTweets=read.csv(trumpLink ,stringsAsFactors = F)
names(allTweets)
head(allTweets$text,2)
DTtweets=allTweets[allTweets$is_retweet==FALSE ,]
DTtweets=DTtweets[,c(1,2)]
#some
head(DTtweets,3)
SomeTrumpTweets=allTweets[allTweets$is_retweet==FALSE ,]
SomeTrumpTweets=SomeTrumpTweets[,c(1,2)]
#some
head(SomeTrumpTweets,3)
library(tidytext)
library(magrittr)
WordsIn_SomeTrumpTweets = SomeTrumpTweets %>%
unnest_tokens(output=EachWord, # column created
input=text,# input column from SomeTrumpTweets
token="words") # level of unnesting
head(WordsIn_SomeTrumpTweets,10) # notice 'EachWord'
nrow(WordsIn_SomeTrumpTweets) # count of words
# calling the file
data(stop_words)
# seeing some 'STOP WORDS'
head(stop_words)
?stop_words
library(dplyr)
# The column 'word' from 'stop_words' will be compared # to the column 'EachWord' in 'WordsIn_SomeTrumpTweets'
WordsIn_SomeTrumpTweets = WordsIn_SomeTrumpTweets %>%anti_join(stop_words,
by = c("EachWord" = "word"))
# You have these many rows now:
nrow(WordsIn_SomeTrumpTweets) # count of words
forCloud=as.data.frame(table(WordsIn_SomeTrumpTweets$EachWord))
names(forCloud)=c('EachWord','Counts')
#sorting by count:
forCloud_ascending=forCloud[order(forCloud$Counts),]
head(forCloud_ascending,10)
tail(forCloud_ascending,10)
# dropping by text:
badWords=c('https','t.co')
forCloud_ascending=forCloud_ascending[!forCloud_ascending$EachWord%in%badWords,]
# dropping by count:
forCloud_ascending=forCloud_ascending[forCloud_ascending$Counts>4,]
library(ggwordcloud)
ggplot(forCloud_ascending,
aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area() +
theme_minimal() +
scale_color_gradient(low = "red", high = "darkred")
ggplot(forCloud_ascending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.65) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.65) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 1) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 2) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.7) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
library(ggplot2)
ggplot(data=forCloud_ascending) + geom_bar(aes(x=EachWord, y=Counts), stat = 'identity')
library(ggplot2)
ggplot(data=forCloud_ascending) + geom_bar(aes(x=reorder(EachWord,Counts), y=Counts), stat = 'identity')
library(ggplot2)
ggplot(data=forCloud_ascending) + geom_bar(aes(x=reorder(EachWord,Counts), y=Counts), stat = 'identity') + coord_flip()
forCloud=as.data.frame(table(WordsIn_SomeTrumpTweets$EachWord))
names(forCloud)=c('EachWord','Counts')
#sorting by count:
forCloud_ascending=forCloud[order(forCloud$Counts),]
head(forCloud_ascending,10)
library(ggplot2)
ggplot(data=forCloud_ascending) + geom_bar(aes(x=reorder(EachWord,Counts), y=Counts), stat = 'identity') + coord_flip()
# dropping by text:
badWords=c('https','t.co')
forCloud_ascending=forCloud_ascending[!forCloud_ascending$EachWord%in%badWords,]
# dropping by count:
forCloud_ascending=forCloud_ascending[forCloud_ascending$Counts>4,]
ggplot(data=forCloud_ascending) + geom_bar(aes(x=reorder(EachWord,Counts), y=Counts), stat = 'identity') + coord_flip()
library(ggwordcloud)
ggplot(forCloud_ascending,
aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area() +
theme_minimal() +
scale_color_gradient(low = "red", high = "darkred")
ggplot(forCloud_ascending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.65) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
forCloud_descending=forCloud_ascending[order(-forCloud_ascending$Counts),]
#now
ggplot(forCloud_descending, aes(label = EachWord, size = Counts,color = Counts)) +
geom_text_wordcloud_area(eccentricity = 0.7) +
theme_minimal() + scale_size_area(max_size = 13)+ # try increasing
scale_color_gradient(low = "red", high = "darkred")
LinkText="https://github.com/DACSS-Visual/basicText/raw/main/sometext.txt"
otherText <- read.delim(LinkText,header = F)
LinkText="https://github.com/DACSS-Visual/basicText/raw/main/sometext.txt"
otherText <- read.delim(LinkText,header = F)
head(otherText)
View(otherText)
otherText_words=otherText %>%
unnest_tokens(output=EachWord,
input=V1,# column of the texts
token="words")%>%
anti_join(stop_words,
by = c("EachWord" = "word"))
# you get
head(otherText_words,20)
txtAsc_descending=dplyr::count(otherText_words,EachWord,
name='Counts',
sort = TRUE)
# result
head(txtAsc_descending,10)
txtAsc_descending=dplyr::count(otherText_words,EachWord,
name='Counts',
sort = TRUE)
txtAsc_descending=txtAsc_descending[txtAsc_descending$Counts>4,]
# result
head(txtAsc_descending,10)
ggplot(txtAsc_descending,
aes(label = EachWord,
size = Counts,color = Counts)) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal() +
scale_color_gradient(low = "red", high = "darkred")
ggplot(txtAsc_descending)  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip()
ggplot(head(txtAsc_descending,15))  +
geom_bar(aes(x=reorder(EachWord,Counts),y=Counts),stat = 'identity') + coord_flip() +
labs(x="") + geom_text(aes(label=paste0(100*round(Counts/sum(Counts),3),'%'),
x=EachWord,
y=Counts),
nudge_y = 2)
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
?rm
rm(list = ls())
##
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
rm(list = ls())
##
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
library(ggplot2)
ggplot(data = usaMap)+ geom_sf()
rm(list = ls())
##
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
library(ggplot2)
ggplot(data = usaMap)+ geom_sf() +coord_sf(xlim = c(120, -60))
rm(list = ls())
##
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
library(ggplot2)
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs("+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs"), datum = NA)
rm(list = ls())
##
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
library(ggplot2)
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs(3467))
rm(list = ls())
##
library(sf)
usaMap=read_sf("gz_2010_us_040_00_500k.json")
library(ggplot2)
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs(2163))
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs(3857))
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs(2946))
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs(4326))
ggplot(data = usaMap)+ geom_sf() +coord_sf(crs = st_crs(2163))
variables=read.csv("data_for_map.csv")
View(variables)
variables=read.csv("data_for_map.csv")
head(variables)
library(dplyr)
variables %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp=sum)) ->variablesByRegion
head(variablesByRegion)
library(dplyr)
variables %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp=sum)) ->variablesByRegion
variablesByRegion
View(usaMap)
head(usaMap)
usaMap$STATE==variables$State
usaMap$STATE
usaMap$NAME
usaMap$NAME==variables$State
usaMap$NAME
variables$State
sort(usaMap$NAME)==variables$State
sort(variables$State)
sort(usaMap$NAME)==sort(variables$State)
merge(usaMap,variables, by.x='NAME', by.y='State')
usaMap=merge(usaMap,variables, by.x='NAME', by.y='State')
usaMap
aggregate(usaMap, Region~Prop_no_Comp,sum)
aggregate(data=usaMap, by=Region~Prop_no_Comp,sum)
aggregate(data=usaMap, by=Region~Prop_no_Comp,mean)
library(dplyr)
usaMap %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp=sum)) ->variablesByRegion
variablesByRegion
plot(variablesByRegion)
ggplot(variablesByRegion) + geom_sf()
library(dplyr)
usaMap %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp_sum=sum)) ->variablesByRegion
ggplot(variablesByRegion) + geom_sf(aes(fill=Prop_no_Comp_sum))
library(dplyr)
usaMap %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp_sum=sum)) ->variablesByRegion
ggplot(variablesByRegion) + geom_sf(aes(fill=Prop_no_Comp_sum)) +coord_sf(crs = st_crs(2163))
library(dplyr)
usaMap %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp_sum=sum)) ->variablesByRegion
#newMap
variablesByRegion
library(dplyr)
usaMap %>%
group_by(Region) %>%
summarise_at(vars(Prop_no_Comp), list(Prop_no_Comp_mean=mean)) ->variablesByRegion
#newMap
variablesByRegion
ggplot(variablesByRegion) + geom_sf(aes(fill=Prop_no_Comp_mean)) +coord_sf(crs = st_crs(2163))
View(variablesByRegion)
# clean memory ------------------------------------------------------------
rm(list = ls())
setwd("/Users/JoseManuel/Documents/GITHUBs/PUCP/ASIES/testDashboard")
filename="theFile.csv"
mydata=read.csv(filename)
View(mydata)
head(mydata)
str(mydata)
# deliverable 1 ----------------------------------------------------------
library(ggplot2)
base= ggplot(data=mydata)
del1Draft= base + geom_bar(aes(x=LocaleType))
del1Draft
# save del1Draft ----------------------------------------------------------
saveRDS(del1Draft, file = "del1Draft.rds")
del2Draft= base + geom_histogram(aes(x=Student.Teacher.Ratio))
del2Draft
# save del2Draft ----------------------------------------------------------
saveRDS(del2Draft, file = "del2Draft.rds")
del3Draft= base + geom_point(aes(x=Student.Teacher.Ratio,
y=Free.Lunch))
del3Draft
# save del3Draft ----------------------------------------------------------
saveRDS(del3Draft, file = "del3Draft.rds")
library(sf)
county_map=sf::read_sf("WA_County_Boundaries.geojson")
head(county_map)
head(mydata)
aggregate(data=mydata,Free.Lunch~County,FUN = mean)
head(county_map)
mydataCounty=aggregate(data=mydata,Free.Lunch~County,FUN = mean)
mydataCounty
myMapLunch=merge(county_map,mydataCounty,by.x='JURISDIC_2',"County")
base=ggplot(myMapLunch)
del4Draft=base + geom_sf(aes(fill=Free.Lunch))
del4Draft
# save del4Draft ----------------------------------------------------------
saveRDS(del4Draft, file = "del4Draft.rds")
